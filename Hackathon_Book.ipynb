{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwsAub-VhSQl"
      },
      "source": [
        "# **Galactic Hackathon 2025**\n",
        "# **Smart Urban Mobility Optimizer: AI-Driven Solutions for Sustainable Cities**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EFJFETnd4Uw"
      },
      "source": [
        "##**Objective**\n",
        "The goal of this challenge is to leverage AI and data analysis to address urban mobility challenges by identifying high-traffic congestion areas, optimizing public transport routes and schedules, predicting congestion patterns, and evaluating the environmental impact of travel. By analyzing traffic data, participants will deliver actionable insights and user-friendly solutions that help city planners and stakeholders make informed decisions to improve traffic flow, enhance public transport efficiency, and promote environmental sustainability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjRGJn3NhYZU"
      },
      "source": [
        "##**Problem Statement**\n",
        "Cities around the world are grappling with increasing traffic congestion, inefficient public transport systems, and rising environmental concerns due to urbanization and growing vehicle usage. These challenges call for innovative solutions to optimize traffic flow, improve public transport efficiency, and minimize environmental impact.\n",
        "\n",
        "Using the provided dataset, participants will analyze traffic patterns, evaluate environmental impacts, and develop AI-powered tools to:\n",
        "\n",
        "- Identify congestion hotspots.\n",
        "- Predict traffic levels.\n",
        "- Propose optimized public transport routes and schedules.\n",
        "\n",
        "The aim is to create actionable, user-friendly insights and recommendations that city planners and stakeholders can use to enhance urban mobility and sustainability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsvAQjiAY57s"
      },
      "source": [
        "##**Dataset**\n",
        "The dataset represents traffic and environmental metrics collected from various locations. It is designed to provide insights into urban mobility, congestion, and environmental conditions in different regions. The dataset includes detailed metrics related to traffic density, air quality, and time-related variables. These metrics can help identify patterns in congestion, environmental pollution, and variations in traffic flow during different times of the day or week.\n",
        "\n",
        "The dataset includes traffic and environmental metrics:\n",
        "\n",
        "You have access to this file.\n",
        "(`hackathon_data.csv`)\n",
        "\n",
        "| Column name | Description | Sample Data |\n",
        "|-------------|-----------|---------|\n",
        "| Location |The name of the location where the data was collected. It provides geographic context for the dataset.|'Lajpat Nagar','Saket'|\n",
        "| Traffic Volume |The number of vehicles passing through the area during a given time period. Helps measure the density of vehicle activity at different locations and times.|'1289','4700'|\n",
        "| Passenger Count |The total number of passengers in the location during the recorded time. This reflects traffic density in terms of human movement.|'23686','12187'|\n",
        "| Noise Level |The average noise level in decibels at the location during the recorded period.The noise level measured in decibels (dB) in the area.|'65.4','85.3'|\n",
        "| Average Speed |The average speed of vehicles (in kilometers per hour) traveling through the area during the specified time. Provides insight into traffic flow and congestion levels.|'15.6', '31.3'|\n",
        "| PM2.5 Level |The concentration of fine particulate matter (PM2.5) in the air, measured in micrograms per cubic meter (µg/m³). Indicates air quality and the level of air pollution harmful to respiratory health.|'85.2', '170.7'|\n",
        "| AQI (Air Quality Index) |A standardized index measuring the quality of air in an area. Higher values indicate worse air quality. Helps monitor environmental health and pollution risks.|'72', '150'|\n",
        "| Time of the Day |The time frame during which the data was captured, categorized as Morning, Afternoon, Evening, or Night. Identifies variations in traffic and environmental metrics based on daily time periods.|'Morning', 'Night'|\n",
        "| Day of the Week | The specific day on which the data was collected. Highlights trends based on weekly traffic and environmental patterns. |'Sunday','Friday'|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7rzcSPkBXGD"
      },
      "source": [
        "##**Chatbot**\n",
        "\n",
        "A chatbot is available to provide insights and recommendations for solving the problems you encounter. You can interact with this chatbot to gain a better understanding of how to solve any problem related to the data. Please ensure your questions focus solely on the dataset and remain on-topic. Instructions on how to use the Chatbot are also provided with the bot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igt5xtbwhYR8"
      },
      "source": [
        "## **Tasks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba8edDginaoQ"
      },
      "source": [
        "### **Task 1: Data Analysis**\n",
        "\n",
        "**Import the dataset**\n",
        "\n",
        "The **hackathon_data.csv** is the file we will perform this hackathon with. To work with this file we must import it first, we can utilize the pandas library for this. Since the file is a .csv file, we can use **pd.read_csv('filename.extension')**.\n",
        "\n",
        "Note: The **Try-except** statement, here try block lets you test a block of code for errors. The except block lets you handle the error.\n",
        "- **FileNotFoundError**: This block of code handles the FileNotFoundError exception.  This exception is raised if the specified file 'hackathon_data.csv' does not exist in the current working directory.\n",
        "- **pd.errors.ParserError**: This block handles the pd.errors.ParserError exception.  This exception is raised by pandas if there's a problem parsing the CSV file (e.g., incorrect formatting, inconsistent delimiters).\n",
        "- **Exception as e**: # This is a general exception handler. It catches any other unexpected errors that might occur during the file reading process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_KXFwu2eqgG",
        "outputId": "55949e8b-23af-4ce7-d43d-80f2a03dc34b"
      },
      "outputs": [],
      "source": [
        "# import pandas library as pd\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a DataFrame.\n",
        "try:\n",
        "# Replace the space with the dataset file path along with the extension csv\n",
        "  df = pd.read_csv('data.csv')\n",
        "\n",
        "# dataframe.head() displays the first 5 rows of the dataset\n",
        "  print(df.head())\n",
        "except FileNotFoundError:\n",
        "  print(\"Error: hackathon_data not found. Please check the file path.\")\n",
        "except pd.errors.ParserError:\n",
        "  print(\"Error: Could not parse hackathon_data. Please check the file format.\")\n",
        "except Exception as e:\n",
        "  print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlS3uJ5BhX0E"
      },
      "source": [
        "### ***Summary of the dataset***\n",
        "\n",
        "The summary of the dataset provides a quick overview of its structure, content, and quality. It helps to understand the Data (Learn about the column names, data types, number of rows, and basic statistics), Identify Issues (Spot missing values, outliers, or inconsistencies), Assess Distribution (Understand how data is distributed, such as skewness or clustering)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm3xRLR1aHXQ",
        "outputId": "19ca428d-ee92-49b6-fe4a-e15398f8d3f0"
      },
      "outputs": [],
      "source": [
        "# .shape returns a tuple representing the dimensions of our DataFrame df. The output shows the number of rows and columns in the dataset, helping to quickly understand its size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G-I4C_DfUV5"
      },
      "source": [
        "So, our dataset has 7000 rows and 9 columns of data.\n",
        "\n",
        "Let us now have a look at the descriptive statistics (Assess Distribution) of the dataset. We will utilize the **.describe** function which generates descriptive statistics for the DataFrame, including **all** data types (numerical, categorical, etc.). It provides information like count, mean, min, max, and unique values, offering a summary of the dataset's distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "6Qb1sCKhZ1Nq",
        "outputId": "de45f7ed-7fd8-4549-b18b-c3c44bde3802"
      },
      "outputs": [],
      "source": [
        "# Display summary statistics of the DataFrame\n",
        "# The 'all' here basically signals the function to return all forms of descriptive statistics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3fAuiTQhnYh"
      },
      "source": [
        "Now, let's have a look at the **info** of the dataframe, including the data types, number of non-null values in each column, and memory usage. It helps to quickly assess the structure and completeness of the dataset. It provides essential information about the dataset, including its structure, column data types, and null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZvjMHXLnaCf",
        "outputId": "3f02d908-40db-4e9c-fee1-2534d6bb739f"
      },
      "outputs": [],
      "source": [
        "# Display data types of each column\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmPkotOe0Xfa"
      },
      "source": [
        "### ***Percentage of null values for each column***\n",
        "\n",
        "We can clearly notice there are some null values in few of our columns, so why don't you find the count of missing values in each column.\n",
        "\n",
        "Hint: **sum()** function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIAcwDuHd4D-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywg5Lrr6i75p"
      },
      "source": [
        "Determine the percentage of missing values for each column in a dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoOedlTjagMt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgg3tRqMmrhe"
      },
      "source": [
        "### ***Correlation Matrix Heatmap***\n",
        "\n",
        "A correlation matrix heatmap is a visual representation of the correlation coefficients between variables in a dataset. Correlation measures the strength and direction of the linear relationship between two variables, typically ranging from -1 to +1:\n",
        "\n",
        "- +1: Perfect positive correlation (as one increases, the other increases).\n",
        "\n",
        "- -1: Perfect negative correlation (as one increases, the other decreases).\n",
        "\n",
        "- 0: No linear relationship.\n",
        "\n",
        "Examine relationships between numerical variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "maIFt_u8mpka"
      },
      "outputs": [],
      "source": [
        "# Create a correlation heatmap for all the numerical columns in the dataset\n",
        "\n",
        "# Visualization libraries have already been installed, you can choose a plot of your choice from either of the libraries\n",
        "\n",
        "# Create the heatmap with specifications (cmap = 'RdYlGn', annot = True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZxbW0Jb0YbS"
      },
      "source": [
        "### ***Boxplot of the dataset***\n",
        "\n",
        "A boxplot (or box-and-whisker plot) is a graphical representation of the distribution of a dataset. It displays key summary statistics and highlights outliers. The box represents the interquartile range (IQR), which contains the middle 50% of the data, and the whiskers extend to the data's range (excluding outliers).\n",
        "\n",
        "- Points outside the whiskers indicate outliers, which could be errors or extreme but valid values.\n",
        "- Multiple boxplots side by side allow you to compare the distributions of different variables or categories.\n",
        "- A skewed box (longer whisker or unbalanced median line) suggests non-normal data, guiding the need for transformations or adjustments.\n",
        "- The size of the box (IQR) indicates variability in the data. A large box means high variability, while a small box suggests consistency.\n",
        "\n",
        "The aim here is to detect and analyze outliers in a dataset with a focus on numeric columns. Also, to visualize the outliers using box plots.\n",
        "\n",
        "You can utilize different methods as per your choice.:\n",
        "-  Z-Score Method\n",
        "-  Interquartile Range (IQR) Method\n",
        "-  Modified Z-Score Method\n",
        "-  Percentile Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iqzBCZR6mhRc"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2adYZ8V1J7V"
      },
      "source": [
        "### ***Outlier Removal***\n",
        "\n",
        "Remove extreme values to ensure robust analysis. Utilize any method of your choice for outlier removal.\n",
        "\n",
        "Why removing outliers is better than deleting the entire row:\n",
        "- Prevents outliers from skewing statistical calculations (mean, variance) or model predictions.\n",
        "- Unlike deleting rows with outliers, capping retains most of the data while reducing the impact of extremes.\n",
        "- Helps machine learning algorithms (especially sensitive ones like linear regression) to better generalize by limiting the influence of outliers.\n",
        "\n",
        "\n",
        "Note: You can also utilize other methods like:\n",
        "- 1. Capping Method: Limit extreme values by capping them to a specific range, such as the 5th and 95th percentiles, to reduce their impact without removing data.\n",
        "- 2. Removing Outliers - Eliminate rows with values that lie beyond a certain threshold.\n",
        "- 3. Transformation Methods - Apply transformations to reduce the influence of outliers.\n",
        "- 4. Model-Based Approaches - Use machine learning models to detect and handle outliers (Isolation Forest, One-Class SVM)\n",
        "- 5. Descriptive Statistic Method - Replace numerical outliers with Median or Mean and the categorical outliers with mode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VitEfphHGZCx"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvn7vsNk0X4K"
      },
      "source": [
        "### ***Impute the null values***\n",
        "Now have a look at the null values in each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2zSy2mn-yNF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpO8gqN1qC7h"
      },
      "source": [
        "Why we need to fill in null values (or missing data):\n",
        "- Null values can distort calculations such as averages, sums, or standard deviations, leading to inaccurate insights.\n",
        "- Machine learning algorithms generally cannot handle null values directly.\n",
        "- Simply dropping rows or columns with null values can lead to significant data loss.\n",
        "- Leaving null values can introduce bias if some algorithms or analyses exclude them automatically.\n",
        "\n",
        "\n",
        "Fill missing data using suitable methods like mean, median, or predictive imputation.\n",
        "\n",
        "- For numerical columns, you can fill in the null values with the mean or median or predictive imputation of the entire column.\n",
        "- For categorical column, Fill missing values with the mode of that column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2kHVcp-1Jgm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEvmGzhLpixl"
      },
      "source": [
        "### ***Datatype Format***\n",
        "\n",
        "A datatype is a classification that specifies which type of value a variable or object can hold in a programming language. It defines the nature of data and determines what operations can be performed on that data.\n",
        "\n",
        "Some common formats include: Integer, Floating Point, String, Boolean, Date/Time, Null, List."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oVr_ciwPpiHe"
      },
      "outputs": [],
      "source": [
        "# Print data type of each column\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJvD7UYkvW0n"
      },
      "source": [
        "**Column datatype changes**\n",
        "\n",
        "- The columns in the dataset already have assigned data types.\n",
        "\n",
        "- You need to modify these data types based on the requirements of the model.\n",
        "\n",
        "- Some columns need to be converted to strings, while others must be converted to integers without any decimal places.\n",
        "\n",
        "- Numerical columns can be converted to floats with a specified number of decimal places (e.g., rounding to 2 decimal places to limit precision to a reasonable level and improve readability). You can also choose to round to 3 decimal places or even 0, depending on your needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Wxi84LTmAYhg"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "# Print data types to verify\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG9hmVWsaR4d",
        "outputId": "f65bd844-2f49-40fa-df64-9ad47ad0a288"
      },
      "outputs": [],
      "source": [
        "# Have a look at the shape of the dataset before you proceed with the basic questions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbej_lG51Z5M"
      },
      "source": [
        "### **Basic Questions Post-Preprocessing**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U2hrhQS1ats"
      },
      "source": [
        "#### **Highlight the top 5 locations with high AQI**\n",
        "Air quality is an important factor to monitor. Can you pinpoint the locations where the air quality is the poorest? Rank the top 5 locations by AQI from highest to lowest. (Output the Location and its AQI)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7UUk7tlfna-X"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0ISBwoQ1bW1"
      },
      "source": [
        "#### **List all locations with their Average Speed**\n",
        "Speed matters, especially when analyzing traffic patterns or travel efficiency. Can you list each location along with its average speed? This will help us understand the movement dynamics at different places. (Output the Locations and their average speed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rIGaL82r1bqU"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHGvATnZ1b2k"
      },
      "source": [
        "#### **Which Location Experiences the Highest Noise Level During the Night?**\n",
        "Noise pollution is a growing concern, particularly at night. Find the location that has the highest noise level during the nighttime. (Output the Location and its noise level in db)\n",
        "\n",
        "What could this indicate about the area? ()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vV2AJtTw1cZ0"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-dPWEt4d0BrA"
      },
      "outputs": [],
      "source": [
        "# What could this indicate about the area?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWXUct_C1cn8"
      },
      "source": [
        "#### **Which Location Has the Lowest PM2.5 Level and what is its AQI?**\n",
        "Cleaner air is a sign of better environmental quality. Let’s identify the place with the cleanest air. Can you find the location with the lowest PM2.5 level and AQI, signifying the healthiest air quality? (Output the Location, its PM2.5 Level and its AQI)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fqlNQR-q1c51"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3uP_5zW1dLs"
      },
      "source": [
        "#### **What is the Average Noise Level Across All Locations?**\n",
        "\n",
        "Noise levels can affect well-being and productivity. What is the average noise level across all locations? This question will give us insight into the general noise pollution levels across the dataset. (Output the Location, its Average Noise Level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gr7cbLm_1dnW"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k0JQsG61d5U"
      },
      "source": [
        "#### **On Which Day of the Week Does the PM2.5 Level Reach Its Lowest Average?**\n",
        "The air quality can vary depending on the day of the week, possibly due to different factors like traffic or weather. Can you find the day of the week with the lowest average PM2.5 level? This will give us insight into when the air is at its cleanest during the week. (Output the Day of the Week)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "A9fQWRnX1eJM",
        "outputId": "665e24a1-a24a-4abc-b2cc-ec69e096dfa6"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M363ANKBDAz"
      },
      "source": [
        "### **Task 2: Advanced Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6DZxp6vLR0B"
      },
      "source": [
        "#### **Calculate the average traffic volume for locations with a passenger count above 30,000**\n",
        "\n",
        "In cities, areas with over 30,000 passengers often encounter unique traffic challenges. Analyze the dataset to calculate the average traffic volume in these densely populated zones. What trends emerge, and how could they guide smarter urban mobility planning and infrastructure upgrades?\n",
        "\n",
        "(Output: Average Traffic Volume)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3XTWryhQLRU7"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWOlDyWzLSST"
      },
      "source": [
        "#### **Identify the location with the highest and lowest average speed**\n",
        "Identifying locations with the highest and lowest average speeds provides critical insights into urban traffic dynamics. High speeds might indicate efficient road design, while low speeds could highlight congestion hotspots. Analyzing these extremes helps uncover patterns in traffic flow, infrastructure efficiency, and potential problem areas, ultimately driving improvements in city mobility. Can you pinpoint the locations with the highest and lowest average speed which drives the impact on city mobility. Also mention their respective average speed.\n",
        "\n",
        "(Output: Location, Average Speed (Highest), Location, Average Speed (Lowest))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yoDufA0nBFCT"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD1bpqTmLSxK"
      },
      "source": [
        "#### **Environmental Correlation**\n",
        "\n",
        "Environmental factors, such as Air Quality Index (AQI), Noise Levels, and Particulate Matter (PM2.5), often interact with one another in complex ways. These relationships can significantly impact urban living conditions and quality of life. For this task, perform an correlation analysis to explore and uncover potential patterns between AQI, Noise Levels, and PM2.5 levels across various locations.\n",
        "\n",
        "- **0**: A correlation coefficient of 0 indicates no relationship between the two variables. This means changes in one variable do not systematically affect the other.\n",
        "- **-1**: A correlation coefficient of -1 indicates a perfect negative relationship. This means as one variable increases, the other variable decreases in a perfectly consistent way.\n",
        "- **+1**: A correlation coefficient of +1 indicates a perfect positive relationship. This means as one variable increases, the other variable also increases in a perfectly consistent way.\n",
        "\n",
        "Ex: To explore these relationships, perform a correlation analysis using columns that represent key environmental aspects. For example, if the correlation between Traffic Volume and Noise Level is 0.7, it indicates that as traffic volume increases, noise levels tend to rise as well.\n",
        "\n",
        "(Output: Matrix Table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RyTlSZ1DBE-r"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvEczHNgLTD6"
      },
      "source": [
        "#### **Group locations by \"Time of the Day\" and calculate average AQI**\n",
        "\"Group locations by 'Time of the Day' and calculate average AQI\"\n",
        "Air quality fluctuates throughout the day due to multiple factors such as traffic patterns, industrial emissions, and weather conditions. Group the locations according to different times of the day, such as morning, afternoon, evening, etc., and compute the average AQI for each time period.\n",
        "\n",
        "(Output: Time of the Day and respective AQI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PoIM2xl2BE1D"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le5_xL1OoOL1"
      },
      "source": [
        "What might be contributing to these fluctuations in air quality, and how can this information be used to guide effective environmental monitoring and mitigation strategies?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "JVwZUYHuoLNH"
      },
      "outputs": [],
      "source": [
        "# Write your opinion for the above insight here...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcK3eTx9LTTS"
      },
      "source": [
        "#### **Calculate traffic density for all locations**\n",
        "Traffic density is a key indicator of congestion and road efficiency. We want to understand how the traffic density changes by location. This metric helps understand how congested or fluid traffic is in various areas.\n",
        "\n",
        "Create the Traffic density metric and identify locations with the highest average and lowest average traffic densities and consider factors that might be influencing these values, such as road capacity, time of day, or weather conditions.\n",
        "\n",
        "(Output: Location, Average Traffic Density)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dVjEnc_FBEqE"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gG_119XLT9T"
      },
      "source": [
        "#### **Determine the most/least active locations by Traffic Volume and Passenger Count**\n",
        "To evaluate the most and least active locations, examine both traffic volume and passenger count. The most active areas will exhibit high traffic and passenger counts, while the least active areas will show low counts. These extremes offer critical insights into urban mobility dynamics, indicating areas where transportation demand significantly influences daily movement patterns. Such locations are essential targets for strategic infrastructure investments, such as expanding road networks, improving public transit accessibility, or implementing traffic management solutions to enhance mobility efficiency.\n",
        "\n",
        "(Output: Most active location by traffic volume and its traffic volume, Least  active location by traffic volume and its traffic volume, Most active location by passenger count and its traffic volume, Least  active location by passenger count and its traffic volume)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Ljf0GnJ2BElr"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_9IlsHTMCzK"
      },
      "source": [
        "### **Task 3: AI Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YRY6oJjI1np"
      },
      "source": [
        "#### **Task 3.1: Data Preprocessing for Machine Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdTtBzukJEu7"
      },
      "source": [
        "**3.1.1 Encode Categorical Data**\n",
        "\n",
        "- Why: Machine learning models typically require numerical inputs. Encoding categorical variables makes them compatible for training.\n",
        "- What to Do:\n",
        "  - Identify categorical columns such as 'Time of the Day', 'Day of the Week', or 'Location'.\n",
        "  - Use encoding techniques like LabelEncoder or OneHotEncoder to convert these into numerical formats.\n",
        "  - Save the encoders for consistent application during deployment or evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "n0_xs8m-0i3v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "FKWUo6hg0a8_"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "m5u3XQ2ZsPF4"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfPUBharJO4g"
      },
      "source": [
        "**3.1.2 Scale Feature Columns**\n",
        "\n",
        "- Why: Scaling numerical features ensures uniform ranges, improving model performance and convergence.\n",
        "- What to Do:\n",
        "  - Identify numerical columns like 'Traffic Volume', 'Passenger Count', 'Noise Level', 'Average Speed', 'PM2.5 Level', and 'AQI'.\n",
        "  - Apply scaling techniques such as StandardScaler.\n",
        "  - Save the scaler object for consistent preprocessing in future data applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0Y9RoixOsSYX"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq1-VMN0I-Ig"
      },
      "source": [
        "#### **Task 3.2: Preparing the Dataset for Model Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJEyhQqAJcMY"
      },
      "source": [
        "**3.2.1  Feature-Target Separation**\n",
        "\n",
        "Separate the dataset into:\n",
        "  \n",
        "- Why: Dividing the dataset into features and targets helps the model clearly distinguish between inputs and outputs.\n",
        "- What to Do:\n",
        "  - Assign the target column, such as 'Traffic Volume', to the variable yyy.\n",
        "  - Use other columns like 'Passenger Count', 'Noise Level', and 'Average Speed' as features XXX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "iTcobHe_JbnJ"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM_yad2wJ0Oo"
      },
      "source": [
        "3.2.2 Split Data for Training and Testing\n",
        "\n",
        "- Why: Splitting the dataset evaluates how well the model generalizes to new, unseen data.\n",
        "- What to Do:\n",
        "  - Use an 80:20 ratio to split the dataset into training and testing subsets.\n",
        "  - Ensure both subsets maintain data integrity and distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "HhaJjOPOs5L-"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxCebCOQKCfY"
      },
      "source": [
        "**3.2.3 Identify Target Variable Types**\n",
        "\n",
        "- Why: The type of target variable determines the model type (regression or classification).\n",
        "- What to Do:\n",
        "  - Check if 'Traffic Volume' is numerical (for regression) or categorical (for classification).\n",
        "  - Choose appropriate models accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "YgI40irGJbhu"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fvf0DZjI-yw"
      },
      "source": [
        "#### **Task 3.3: Training Machine Learning Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-rR1WduKSZI"
      },
      "source": [
        "**3.3.1 Model Selection and Initialization**\n",
        "\n",
        "- Why: Selecting a suitable model aligns with the dataset and problem type.\n",
        "- What to Do:\n",
        "  - Use regression models (e.g., Linear Regression, Random Forest Regressor) for numerical targets like 'Traffic Volume'.\n",
        "  - Use classification models (e.g., Logistic Regression, Decision Trees, SVC) for categorical targets (if applicable)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "IAQQURvqIspp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqyoudhjKgW_"
      },
      "source": [
        "**3.3.2 Train the Models**\n",
        "- Why: Proper training enables models to learn patterns in the dataset.\n",
        "- What to Do:\n",
        "  - Train models using the training subset.\n",
        "  - Monitor for stability and convergence during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "r-zfvRxhtDy1"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CslHIQesKkAH"
      },
      "source": [
        "**3.3.3 Evaluate Model Performance**\n",
        "\n",
        "- Why: Evaluation metrics assess the model’s accuracy and reliability on unseen data.\n",
        "- What to Do:\n",
        "  - For classification models, compute metrics like accuracy, precision, recall, and F1 Score.\n",
        "  - For regression models, calculate metrics such as Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R² Score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ImGMMgsDtEOl"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "GdLks6-L5PGO"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkyoL4qgLX_R"
      },
      "source": [
        "### Task 4: Report Creation\n",
        "Report creation is a critical component of any hackathon because it serves as a comprehensive summary of the team's journey, insights, and outcomes. While the technical solutions or models built during the event are essential, a well-crafted report translates complex findings into actionable and understandable information for various stakeholders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3uxXu7DMKyI"
      },
      "source": [
        "Reports should effectively summarize the findings, methodologies, and insights gained during the hackathon tasks. The report will be a PowerPoint presentation covering the following topics. Here's a suggested structure for the report:\n",
        "\n",
        "Table of Contents\n",
        "1. Executive Summary\n",
        "2. Introduction\n",
        "3. Methodology and Approach\n",
        "4. Key Findings\n",
        "   - Data Preprocessing\n",
        "   - Exploratory Data Analysis (EDA)\n",
        "   - AI Task Results\n",
        "5. Insights and Recommendations\n",
        "6. Conclusion and Future Work\n",
        "7. Appendices\n",
        "\n",
        "Note: For better understanding of each heading above, you can refer the Hackathon Handbook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTk4r1KqMJ2v"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
